A project to implement data engineering concepts  

Purpose: To ingest, process and analyze data logs from e-commerce logs.   

Softwares used:  
Programming Languages: Python, SQL  
Databases: MySQL  
Frameworks: Hadoop, Spark  
API: PySpark  
Version Control: Git  


Metadata Schema  


|field         |data_type|description                                                                         |  
|:-------------|:--------|:-----------------------------------------------------------------------------------|  
|user_id       |string   |Stores the User ID of the customer                                                  |  
|type_of_event |string   |Mentions the type of event based on what happened. eg: view, cart, purchase, return |  
|event_timestamp|datetime|Timestamp at which the event occured                                                |  
|product_id    |string   |Unique ID of the product for which the specified event occured for the customer     |  
|product_revenue|float   |Revenue of the product for the specified event                                      |  


For product_revenue:  
If purchase event, value will be positive, reflecting profit to company  
If return event, value will be negative, reflecting loss to company  
If event view or cart, value will be zero, indicating no revenue change with these actions to the company  

Process:  

Phase 0:  **Initialization**

Created the initial project structure and created Git and GitHub repository.  

Phase 1:  **Basic Python Script**

Generated random logs onto a comma-separated file (logs/random_{yesterday_date}.csv) using a python script (data-generator/log-generator.py).  

Phase 2:  **Intermediate Python, PySpark and Spark SQL**

Using a PySpark script (pyspark-jobs/process_logs.py), converted raw logs into analytical tables, which were saved as .parquet files.
Analytical tables are as follows:  


1. **product_count** 


|field                      |data_type|description                                                                         |  
|:--------------------------|:--------|:-----------------------------------------------------------------------------------|  
|product_id                 |string   |Stores the product ID of the ecommerce log.                                         |  
|cart                       |int      |Gets the total cart save count that occured for a specified product ID.             |  
|purchase                   |int      |Gets the total purchase count that occured for a specified product ID.              |  
|return                     |int      |Gets the total return count that occured for a specified product ID.                |  
|view                       |int      |Gets the total view count that occured for a specified product ID.                  |  
|event_date                 |date     |Collects the date of event.                                                         |  
|product_event_count        |int      |Gets the total count of all events occured for a specified product ID.              |  
|total_product_revenue      |double   |Collects the total revenue of the specified product_id.                             |  
|product_price              |double   |Gets the price of the product.                                                      |


2. **event_count**


|field                      |data_type|description                                                                         |  
|:--------------------------|:--------|:-----------------------------------------------------------------------------------|
|event_date                 |date     |Date of event (useful for later)                                                    |  
|type_of_event              |string   |Stores the specific event that happened                                             |  
|count_of_event             |int      |Gets the total count of specified events                                            | 
|event_revenue              |string   |Total revenue generated by event                                                    | 



3. **time_events** 


|field                      |data_type|description                                                                                  |   
|:--------------------------|:--------|:-----------------------------------------------------------------------------------         |   
|time_interval              |date     |Time interval, divided into six hours                                                        |  
|cart                       |int      |Gets the total count of cart storage events that happened on a specified time interval.      |  
|purchase                   |int      |Gets the total count of purchase events that happened on a specified time interval.          |  
|return                     |int      |Gets the total count of return events that happened on a specified time interval.            |  
|view                       |int      |Gets the total count of items just being viewed on a specified time interval.                |  
|event_date                 |double   |Collects date of event from logs.                                                            |  
|total_event_count          |int      |Collects the total amount of events occured for a specified time interval.                   |  
|total_revenue              |double   |Gets the total revenue collected on a specified time interval.                               |  



Phase 4: **HDFS and Hive with HiveQL**

Transferred the output PARQUET files to HDFS, sorted by table and partitioned by date. Created folders if not created.

Created external tables within Hive as part of the 'ecommerce' database to perform HiveQL queries on it. Needed to install Tez to process complex queries


Phase 5: **Airflow Workflow Orchestration**

Created an Airflow DAG script in Python to orchestrate the whole workflow in an orderly manner. Records are updated by date, newer runs for the same date replace old records with the same date.


**CHANGES**


*07-Sep-2025*:  
Added new columns to product_count table to better reflect real life situations. Removed one column(product_user_impressions) because of irrelevancy.

*21-Nov-2025*:
Replaced *"date_events"* with *"time_events"* to better reflect daily updation of records. Created Airflow script for orchestration. Updated both the log-generator file and log-processor scripts for Airflow idempotency.